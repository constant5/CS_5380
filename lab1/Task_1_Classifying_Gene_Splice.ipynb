{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSCE 5380 Spring 2018 - Data Mining - Lab Assignment 1  - Task 1\n",
    "\n",
    "Constant Marks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Index<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Problem-1:-Cross-validation\" data-toc-modified-id=\"Problem-1:-Cross-validation-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Problem 1: Cross-validation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Question-1.\" data-toc-modified-id=\"Question-1.-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Question 1.</a></span></li><li><span><a href=\"#Question-2\" data-toc-modified-id=\"Question-2-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Question 2</a></span></li><li><span><a href=\"#Question-3.\" data-toc-modified-id=\"Question-3.-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Question 3.</a></span></li></ul></li><li><span><a href=\"#Problem-2:-Percentage-Split\" data-toc-modified-id=\"Problem-2:-Percentage-Split-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Problem 2: Percentage Split</a></span><ul class=\"toc-item\"><li><span><a href=\"#Question-1.\" data-toc-modified-id=\"Question-1.-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Question 1.</a></span></li><li><span><a href=\"#Question-2.\" data-toc-modified-id=\"Question-2.-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Question 2.</a></span></li><li><span><a href=\"#Question-3.\" data-toc-modified-id=\"Question-3.-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Question 3.</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first run the Na√Øve Bayes classifier with 3 different numbers of folds (5, 10, 15 folds in each run respectively) and then run the Random Forest classifier by selecting the same number of folds. Next we will choose a 10% test set split, fit the classifiers, predict the results of the tes set, and compare the predicted results to the known values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import the data and inspect the first record in the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(b'ATRINS-DONOR-521', b'C', b'C', b'A', b'G', b'C', b'T', b'G', b'C', b'A', b'T', b'C', b'A', b'C', b'A', b'G', b'G', b'A', b'G', b'G', b'C', b'C', b'A', b'G', b'C', b'G', b'A', b'G', b'C', b'A', b'G', b'G', b'T', b'C', b'T', b'G', b'T', b'T', b'C', b'C', b'A', b'A', b'G', b'G', b'G', b'C', b'C', b'T', b'T', b'C', b'G', b'A', b'G', b'C', b'C', b'A', b'G', b'T', b'C', b'T', b'G', b'EI') \n",
      "\n",
      "There are 3190 records: \n"
     ]
    }
   ],
   "source": [
    "from scipy.io.arff import loadarff\n",
    "with open('data/splice.arff', 'r') as f:\n",
    "    data, meta = loadarff(f)\n",
    "print(data[0],'\\n')\n",
    "print('There are %d records: ' % (data.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Next we slice out the feature set, encode the values, and cast the values to integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  [b'A' b'C' b'D' b'G' b'N' b'R' b'S' b'T']\n",
      "Encoded to:  [0 1 2 3 4 5 6 7]\n"
     ]
    }
   ],
   "source": [
    "X = data[meta.names()[1:-1]]\n",
    "import numpy as np\n",
    "X = np.asarray(X.tolist())\n",
    "features = np.unique(X)\n",
    "print('Features: ',features)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder_X = LabelEncoder()\n",
    "labelencoder_X.fit(features)\n",
    "for i in range(np.shape(X)[1]):\n",
    "    X[:,i] = labelencoder_X.transform(X[:, i])\n",
    "X = X.astype(int)\n",
    "print('Encoded to: ',np.unique(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we slice out the dependanet variable, encode the data (which is not required for these models but may be useful later) and count the freqeuncy of each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Frequencies: \n",
      " [['EI' 'IE' 'N']\n",
      " ['767' '768' '1655']]\n"
     ]
    }
   ],
   "source": [
    "y = data[meta.names()[-1]]\n",
    "labels = np.unique(y).astype(str)\n",
    "labelencoder_y = LabelEncoder()\n",
    "y = labelencoder_y.fit_transform(y)\n",
    "bin_count = np.bincount(y)\n",
    "label_freq = np.array([labels, bin_count])\n",
    "print('Class Frequencies: \\n',label_freq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the Naive Bayes Classifier and perform k-fold cross validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Cross Validation(k=5) Accuracy:  0.91 (+/- 0.01)\n",
      "Naive Bayes Cross Validation(k=10) Accuracy: 0.91 (+/- 0.03)\n",
      "Naive Bayes Cross Validation(k=10) Accuracy: 0.91 (+/- 0.03)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "nb_classifier = GaussianNB()\n",
    "nb_scores_5 = cross_val_score(nb_classifier, X, y, cv=5)\n",
    "nb_scores_10 = cross_val_score(nb_classifier, X, y, cv=10)\n",
    "nb_scores_15 = cross_val_score(nb_classifier, X, y, cv=15)\n",
    "\n",
    "print(\"Naive Bayes Cross Validation(k=5) Accuracy:  %0.2f (+/- %0.2f)\" % (nb_scores_5.mean(), nb_scores_5.std() * 2))\n",
    "print(\"Naive Bayes Cross Validation(k=10) Accuracy: %0.2f (+/- %0.2f)\" % (nb_scores_10.mean(), nb_scores_10.std() * 2))\n",
    "print(\"Naive Bayes Cross Validation(k=10) Accuracy: %0.2f (+/- %0.2f)\" % (nb_scores_15.mean(), nb_scores_15.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we create the Random Forest Classifier and perform k-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Cross Validation(k=5) Accuracy:  0.94 (+/- 0.01)\n",
      "Random Forest Cross Validation(k=10) Accuracy: 0.95 (+/- 0.03)\n",
      "Random Forest Cross Validation(k=10) Accuracy: 0.95 (+/- 0.04)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_classifier = RandomForestClassifier(n_estimators = 100, criterion = 'entropy')\n",
    "rf_scores_5 = cross_val_score(rf_classifier, X, y, cv=5)\n",
    "rf_scores_10 = cross_val_score(rf_classifier, X, y, cv=10)\n",
    "rf_scores_15 = cross_val_score(rf_classifier, X, y, cv=15)\n",
    "\n",
    "print(\"Random Forest Cross Validation(k=5) Accuracy:  %0.2f (+/- %0.2f)\" % (rf_scores_5.mean(), rf_scores_5.std() * 2))\n",
    "print(\"Random Forest Cross Validation(k=10) Accuracy: %0.2f (+/- %0.2f)\" % (rf_scores_10.mean(), rf_scores_10.std() * 2))\n",
    "print(\"Random Forest Cross Validation(k=10) Accuracy: %0.2f (+/- %0.2f)\" % (rf_scores_15.mean(), rf_scores_15.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the number of folds have any correlation with the number and percentage of correctly classified instances within the same model (For example, 5 folds and 10 folds in NB and RF respectively)? Explain the results.\n",
    "\n",
    "   **Answer:** Yes by selecting a smaller number of folds the accuracy of the Naive Bayes classifier does increase slightly (but not statically significant).  This is because with a smaller number of folds we have a larger training set to fit the model, and with a probalistic model this will typically increase the accuracy wthout overfitting.  \n",
    "    \n",
    "   With the Random Forest Classifier, however, the accuracy does not increase with increasing folds and may be even be degraded, indicating that the Random Forest model is overfitting with increased training set size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same number of folds when applied to different models have any effect on the number and percentage of correctly classified instances (For example, 5 folds and 10 folds in NB and RF)? Explain the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  3.19746840e-05   2.89503850e-04   3.05751038e-04]\n",
      "[  3.93652691e-05   1.56835008e-04   3.89675500e-04]\n",
      "[  7.39058512e-06   1.32668842e-04   8.39244621e-05]\n"
     ]
    }
   ],
   "source": [
    "nb_variance = np.array([nb_scores_5.var(), nb_scores_10.var(), nb_scores_15.var()])\n",
    "print(nb_variance)\n",
    "rf_variance = np.array([rf_scores_5.var(), rf_scores_10.var(), rf_scores_15.var()])\n",
    "print(rf_variance)\n",
    "d=np.abs((nb_variance-rf_variance))\n",
    "print(d)\n",
    "mean_d = d.mean()\n",
    "k = d.size\n",
    "difsumsq_d = np.sum((d - mean_d)**2)\n",
    "varsq_d = difsumsq_d/(k/(k-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   **Answer:**Yes, clearly the models will behave differently even with the same number of folds because they are based on different classification algorithms. Also as mentioned above, increasing the number of folds does not improve the accuracy of the Naive Bayes model but may increase the accuracy of the Random Forest model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "Select 1 set of results generated for each classifier. For example, if you performed a test by selecting 10 folds, select the results you obtained for 10 folds for both‚ÄìNB and RF. Considering all classes in the dataset; calculate the accuracy and error rate for the results of NB and RF. Show the formula and explain the steps in calculating the accuracy and error-rate. **Hint**: compute the values of the confusion matrix first.\n",
    "\n",
    "Now we will select a 10% test split for each model, fit the models to the training set, predict the results of the test set,  and evaluate the accuracy of the predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Naive Bayes Classifier with a 10% test split has a single test accuracy rate of 0.928,\n",
      " an error rate of 0.072, and a k-fold cross validation score of 0.912 (+/- 0.03)\n",
      "\n",
      "The Random Forest Classifier with a 10% test split has a single test accuracy rate of 0.953,\n",
      " an error rate of 0.047, and a k-fold cross validation score of 0.944 (+/- 0.02)\n"
     ]
    }
   ],
   "source": [
    "# Splitting data into test and training set choosing a 10% sample Set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1)\n",
    "\n",
    "# Fitting the Naive Bayes classifier\n",
    "nb_classifier.fit(X_train,y_train)\n",
    "\n",
    "# Predicting the Test set results with the Naive Bayes classifier\n",
    "y_pred = nb_classifier.predict(X_test)\n",
    "\n",
    "# Making the Confusion Matrix and Calculatng Accuracy Naive Bayes classifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "nb_accuracy = (cm[0,0]+cm[1,1]+cm[2,2])/np.sum(cm)\n",
    "nb_error = 1 - nb_accuracy \n",
    "\n",
    "# Format the output\n",
    "print('The Naive Bayes Classifier with a 10%% test split has a single test accuracy rate of %.3f,\\n an error rate of %.3f, and a k-fold cross validation score of %.3f (+/- %0.2f)\\n' % (nb_accuracy, nb_error, nb_scores_10.mean(),nb_scores_10.std()*2))\n",
    "\n",
    "\n",
    "# Fitting the Random Forest classifier\n",
    "rf_classifier.fit(X_train,y_train)\n",
    "\n",
    "# Predicting the Test set results with the Random Forest classifier\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Making the Confusion Matrix and Calculatng Accuracy of the Random Forest classifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "rf_accuracy = (cm[0,0]+cm[1,1]+cm[2,2])/np.sum(cm)\n",
    "rf_error = 1 - rf_accuracy \n",
    "\n",
    "# Format the output\n",
    "print('The Random Forest Classifier with a 10%% test split has a single test accuracy rate of %.3f,\\n an error rate of %.3f, and a k-fold cross validation score of %.3f (+/- %0.2f)' % (rf_accuracy, rf_error, rf_scores_10.mean(),rf_scores_10.std()*2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   **Answer:** From above you can see the Naive Bayes and Random Forest models results with a 10% test split. Where accuracy = (sum correct predictions) / (sum total predictions). The correct predictions are taken from the diagonal of the confusion matrix and the total is the sum of of all the elements in the confusion matrix. The error rate is simply, error =  1 - accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Percentage Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will run the NB anf RF classifiers by with three different percentages of training data: 25%-75%, 50%-50%, and 75%-25%, and determine if the percentage of training data affects the classifier accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Splitting data into three different test and training sets \n",
    "X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(X, y, test_size = .25)\n",
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X, y, test_size = .5)\n",
    "X_train_3, X_test_3, y_train_3, y_test_3 = train_test_split(X, y, test_size = .75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: \n",
      " 25-75\t 50-50\t 75-25 \n",
      " [ 0.892  0.907  0.894]\n",
      "Naive Bayes Error: \n",
      " 25-75\t 50-50\t 75-25 \n",
      " [ 0.108  0.093  0.106] \n",
      "\n",
      "Random Forest Accuracy: \n",
      " 25-75\t 50-50\t 75-25 \n",
      " [ 0.941  0.95   0.93 ]\n",
      "Random Forest Error: \n",
      " 25-75\t 50-50\t 75-25 \n",
      " [ 0.059  0.05   0.07 ]\n"
     ]
    }
   ],
   "source": [
    "# Fitting and evaluating the Naive Bayes classifier with the 25% test set\n",
    "nb_classifier.fit(X_train_1,y_train_1)\n",
    "y_pred = nb_classifier.predict(X_test_1)\n",
    "cm = confusion_matrix(y_test_1, y_pred)\n",
    "nb_accuracy = np.array([(cm[0,0]+cm[1,1]+cm[2,2])/np.sum(cm),0,0])\n",
    "nb_error = np.array([(1 - nb_accuracy[0]),0,0])\n",
    "\n",
    "\n",
    "# Fitting and evaluating the Naive Bayes classifier with the 50% test set\n",
    "nb_classifier.fit(X_train_2,y_train_2)\n",
    "y_pred = nb_classifier.predict(X_test_2)\n",
    "cm = confusion_matrix(y_test_2, y_pred)\n",
    "nb_accuracy[1] = (cm[0,0]+cm[1,1]+cm[2,2])/np.sum(cm)\n",
    "nb_error[1] = 1 - nb_accuracy[1]\n",
    "\n",
    "\n",
    "# Fitting and evaluating the Naive Bayes classifier with the 75% test set\n",
    "nb_classifier.fit(X_train_3,y_train_3)\n",
    "y_pred = nb_classifier.predict(X_test_3)\n",
    "cm = confusion_matrix(y_test_3, y_pred)\n",
    "nb_accuracy[2] = (cm[0,0]+cm[1,1]+cm[2,2])/np.sum(cm)\n",
    "nb_error[2] = 1 - nb_accuracy[2]\n",
    "\n",
    "\n",
    "# Fitting and evaluating the Random Forest classifier with the 25% test set\n",
    "rf_classifier.fit(X_train_1,y_train_1)\n",
    "y_pred = rf_classifier.predict(X_test_1)\n",
    "cm = confusion_matrix(y_test_1, y_pred)\n",
    "rf_accuracy = np.array([(cm[0,0]+cm[1,1]+cm[2,2])/np.sum(cm),0,0])\n",
    "rf_error = np.array([(1 - rf_accuracy[0]),0,0])\n",
    "\n",
    "# Fitting and evaluating the  Random Forest classifier with the 50% test set\n",
    "rf_classifier.fit(X_train_2,y_train_2)\n",
    "y_pred = rf_classifier.predict(X_test_2)\n",
    "cm = confusion_matrix(y_test_2, y_pred)\n",
    "rf_accuracy[1] = (cm[0,0]+cm[1,1]+cm[2,2])/np.sum(cm)\n",
    "rf_error[1] = 1 - rf_accuracy[1]\n",
    "\n",
    "# Fitting and evaluating the  Random Forest classifier with the 75% test set\n",
    "rf_classifier.fit(X_train_3,y_train_3)\n",
    "y_pred = rf_classifier.predict(X_test_3)\n",
    "cm = confusion_matrix(y_test_3, y_pred)\n",
    "rf_accuracy[2] = (cm[0,0]+cm[1,1]+cm[2,2])/np.sum(cm)\n",
    "rf_error[2] = 1 - rf_accuracy[2]\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "print('Naive Bayes Accuracy: \\n 25-75\\t 50-50\\t 75-25 \\n',np.around(nb_accuracy,3))\n",
    "print ('Naive Bayes Error: \\n 25-75\\t 50-50\\t 75-25 \\n',np.around(nb_error,3),'\\n')\n",
    "print('Random Forest Accuracy: \\n 25-75\\t 50-50\\t 75-25 \\n',np.around(rf_accuracy,3))\n",
    "print ('Random Forest Error: \\n 25-75\\t 50-50\\t 75-25 \\n',np.around(rf_error,3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'prettytable'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-89-16aef890fb22>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mprettytable\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPrettyTable\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPrettyTable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfield_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"City name\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Area\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Population\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Annual Rainfall\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'prettytable'"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "x = PrettyTable()\n",
    "\n",
    "x.field_names = [\"City name\", \"Area\", \"Population\", \"Annual Rainfall\"]\n",
    "\n",
    "x.add_row([\"Adelaide\", 1295, 1158259, 600.5])\n",
    "x.add_row([\"Brisbane\", 5905, 1857594, 1146.4])\n",
    "x.add_row([\"Darwin\", 112, 120900, 1714.7])\n",
    "x.add_row([\"Hobart\", 1357, 205556, 619.5])\n",
    "x.add_row([\"Sydney\", 2058, 4336374, 1214.8])\n",
    "x.add_row([\"Melbourne\", 1566, 3806092, 646.9])\n",
    "x.add_row([\"Perth\", 5386, 1554769, 869.4])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the percentage of training data affect the classifier accuracy? How and why?\n",
    "    \n",
    "   **Answer** Yes, the percentage of the training set does effect the classifier accuracy for the same reasons discussed above for k-fold cross validation.  The Naive Bayes classifier improves with a larger training set due to the probalistic combination of the data.  The Random Forest classifier does not improve because with increasing training set size.  This indicates that the optimum tree size is created with only a small portion of the data set and any additional data will cause overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the same percentage of testing data is used for both classifiers (e.g. 40% for both NB and RF, does the classification accuracy vary from one classifier to another? Why?\n",
    "    \n",
    "   **Answer** Yes, if we observe the 50-50 split of the data set, we see that the Naive Bayes and Random Forest classifiers had an 82% and 90% accurary rate respectivey. It is clear that the Naive Bayes classifier needs a larger training set to predict with comparale accuray to the Random Forest classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select any one set of results generated for each classifier. For example, if you performed a test by selecting 40% training data, select the results you obtained for 40% for both ‚Äì NB and RF. Considering all classes in the dataset, calculate the accuracy and error rate for the results of NB and RF. Show the formula and explain the steps in calculating the accuracy and error-rate.\n",
    "    \n",
    "   **Answer** The accuracy and error rate ar shown above for all the splits.  The formula for the accuracy and error rate are discussed in Part 1.  If we consider the 50-50 split, the Naive Bayes accurary rate is 82% and the error rat is 18%, and the Random Forest accuracy rate is 90% and the error rate is 10%."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Index",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
