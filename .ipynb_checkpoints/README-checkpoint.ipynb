{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSCE 5380 Fall 2017 - Data Mining - Lab Assignment 1\n",
    "\n",
    "## Index\n",
    "* [Objectives](#Objectives)\n",
    "* [Tools](#Tools)\n",
    "* [Installation](#Installation)\n",
    "* [Demo](#Demo)\n",
    "* [Tasks](#Tasks)\n",
    "* [Submission](#Submission)\n",
    "* [Resources](#Resources)\n",
    "* [References](#References)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "This lab assignment includes hands-on tasks to help us get used to data analysis and some classification methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Tools\n",
    "We use **Python** (either 2.7.\\* or 3.6.\\*) and **scikit-learn** to do the tasks. If you prefer other tools, please talk to the instructor or the TA.\n",
    "\n",
    "For convenience, we choose **Anaconda**, a data science platform distributed with Python and Python libraries such as NumPy, SciPy, pandas, scikit-learn, TensorFlow, Theano, and so on. You can add more libraries or features by using its package manager named **conda**.\n",
    "\n",
    "Anaconda also comes with **Jupyter Notebook**, which is an open-source web application to create and share documents that contain live code, equations, visualizations, and explanatory text, e.g. the document you are reading.\n",
    "\n",
    "**Note:** Anaconda is kinda big with packages you might never need. You could alternatively install **virtualenv** (which also works on our lab computers without **sudo** permission and on [CSE student server](http://esupport.cse.unt.edu/index.php?news=1)), then install scikit-learn via **pip**. After experimentaion, you can write and upload your .ipynb files to [Github](https://github.com/blog/1995-github-jupyter-notebooks-3), Github will render the notebooks for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "1. Download Anaconda from https://www.anaconda.com/download/.\n",
    "2. Install Anaconda into the directory you have write permission, e.g. your home directory.\n",
    "3. Start Anaconda Navigator. You can find it in the installation directory chosen above.\n",
    "![Anaconda Navigator app icon](imgs/Anaconda_app.png)<br></br>\n",
    "4. From Anaconda Navigator, launch Jupyter Notebook.\n",
    "![Jupyter icon](imgs/Navigator.png)<br></br>\n",
    "5. Jupyter File Explorer should show up in a browser window/tab.\n",
    "![Explorer icon](imgs/File_Explorer.png)\n",
    "5. Using Jupyter File Explorer, go to the directory which contains the document you are reading.\n",
    "6. Using Jupyter File Explorer, open **README.ipynb**. **Jupyter Editor** should show up in a new tab. Please **follow the next steps in Jupyter Editor** from now on.\n",
    "7. Now you should be able to interact with this document. Try to double-click on any part of it. The document consists of smaller parts called **cells**. Each cell can contain Markdown code (for descriptions) or Python code.\n",
    "\n",
    "**Note:** Press H to see the keyboard shortcuts, you will need them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo\n",
    "### Know your current working directory\n",
    "It's always good to know where we are. Double-click the cell with Python code below, then press Ctrl-Enter to run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data from file\n",
    "There are some [small datasets](http://scikit-learn.org/stable/datasets/index.html) already bundled with scikit-learn. You can also find some example datasets from the [Weka collections](http://www.cs.waikato.ac.nz/ml/weka/datasets.html) or the [UCI Repository](http://archive.ics.uci.edu/ml/index.php). The datasets can be stored in different formats, e.g. [ARFF](http://www.cs.waikato.ac.nz/ml/weka/arff.html), CSV, JSON, or SQL files.\n",
    "\n",
    "In the following example, we load the Iris Plants Database from an ARFF file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from scipy.io.arff import loadarff\n",
    "with open(\"data/iris.arff\", \"r\") as f:\n",
    "    data, meta = loadarff(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the description of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the data points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"There are %d of data points:\" % (data.size))\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = data[meta.names()[:-1]]\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have an array contains the features, how about the data types of its elements?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(X.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many scikit's model implementations don't support these complex types **(&lt;feature label&gt;, &lt;feature type&gt;)**. We need to convert the data type of every element into **float** as we know it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.asarray(X.tolist(), dtype=np.float_)\n",
    "print(X.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phew! Let's prepare the labels (outputs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Y = data[meta.names()[-1]]\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "Assuming the features are conditionally independent, we can use a Naïve Bayes classifier. Furthermore, those are real valued features; so if we assume the likelihood of the features follow normal (Gaussian) distributions, we can use a Gaussian Naïve Bayes classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation\n",
    "Assuming that the data is independent and identically distributed (which is rarely found in practice), let's do 4-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(gnb, X, Y, cv=4)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "You are recommended to create a notebook for each task.\n",
    "### Task 1\n",
    "Run two classification algorithms--Naïve Bayes (NB) and Random Forest (RF)--on the [Image Segmentation data](data/segment-challenge.arff).\n",
    "\n",
    "#### Problem 1: Cross-validation\n",
    "Run the Naïve Bayes classifier by 3 different numbers of folds (for example, do 3 different runs by 6, 9, 14 folds in each run respectively).\n",
    "\n",
    "Now, run the Random Forest classifier by selecting the same number of folds selected for the Naïve-Bayes classsifer (for example, if you selected 6, 9, 14 folds for Naïve-Bayes, use the same 3 numbers for Random Forest).\n",
    "\n",
    "1. Do the number of folds have any correlation with the number and percentage of correctly classified instances within the same model (For example, 6 folds and 9 folds in NB and RF respectively)? Explain the results.\n",
    "\n",
    "2. Do the same number of folds when applied to different models have any effect on the number and percentage of correctly classified instances (For example, 6 folds and 9 folds in NB and RF)? Explain the results.\n",
    "\n",
    "3. Select 1 set of results generated for each classifier. For example, if you performed a test by selecting 9 folds, select the results you obtained for 9 folds for both–NB and RF. Considering all classes in the dataset; calculate the accuracy and error rate for the results of NB and RF. Show the formula and explain the steps in calculating the accuracy and error-rate. **Hint**: compute the values of the confusion matrix first.\n",
    "\n",
    "#### Problem 2: Percentage Split\n",
    "Run the NB classifier by selecting 3 different percentages of training data (for example, run 3 different runs by selecting a testing-training split of 42%-58%, 54%-46%, 65%-35%, etc. in each run).\n",
    "\n",
    "Run the RF classifier by selecting the same set of training set ratios selected for the NB classifier. Does the percentage of training data affect the classifier accuracy?\n",
    "\n",
    "1. Does the percentage of training data affect the classifier accuracy? How and why?\n",
    "\n",
    "2. If the same percentage of testing data is used for both classifiers (e.g. 42% for both NB and RF, does the classification accuracy vary from one classifier to another? Why?\n",
    "\n",
    "3. Select any one set of results generated for each classifier. For example, if you performed a test by selecting 42% training data, select the results you obtained for 42% for both – NB and RF. Considering all classes in the dataset, calculate the accuracy and error rate for the results of NB and RF. Show the formula and explain the steps in calculating the accuracy and error-rate.\n",
    "\n",
    "### Task 2\n",
    "Run two classification algorithms--Naïve Bayes (NB) and Random Forest (RF)--on the [Thyroid disease records](data/hypothyroid.arff).\n",
    "\n",
    "#### Problem 3: Cross-validation\n",
    "Run each of the NB and RF classifier once by a certain value for number of folds for cross-validation (e.g. 6 folds). \n",
    "\n",
    "For the results obtained, generate a [ROC curve](http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html) for the class “primary-hypothyroid” for both the classifiers.\n",
    "\n",
    "#### Problem 4: Percentage Split\n",
    "Run each of the NB and RF classifier once by selecting a certain percentage of training data (e.g. 42%).\n",
    "\n",
    "For the results obtained, generate a ROC curve for the class “secondary-hipothyroid” for both the classifiers.\n",
    "\n",
    "### Task 3\n",
    "Run the Decision Tree classifier on [Horse Colic database](data/colic.arff), [Pima Indians Diabetes Database](data/diabetes.arff), and [Thyroid disease records](data/sick.arff) with a 70% and 30% split of training and test data respectively. \n",
    "\n",
    "#### Problem 5:\n",
    "Calculate and report the error-rate and accuracy for each of the datasets. Which of the 3 datasets has the highest number of correctly classified instances?\n",
    "\n",
    "Which of the three datasets has the smallest and largest decision trees? Explain why you think the size of the decision trees varies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "You are encouraged to compose your answers in Jupyter using [Markdown](https://guides.github.com/pdfs/markdown-cheatsheet-online.pdf) and Python. Please put into a directory all of your notebook files (\\*.ipynb), source code files (\\*.py), together with data files (if not too big); and submit it to Blackboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "Scikit-learn's rules of thumb for model choices:\n",
    "* http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html\n",
    "\n",
    "Collections of Jupyter notebooks:\n",
    "* https://github.com/donnemartin/data-science-ipython-notebooks#deep-learning\n",
    "* https://github.com/jupyter/jupyter/wiki/A-gallery-of-interesting-Jupyter-Notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "* Fisher, R.A., \"The use of multiple measurements in taxonomic problems\", Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to Mathematical Statistics\" (John Wiley, NY, 1950).\n",
    "* Eibe Frank, Mark A. Hall, and Ian H. Witten (2016). The WEKA Workbench. Online Appendix for \"Data Mining: Practical Machine Learning Tools and Techniques\", Morgan Kaufmann, Fourth Edition, 2016.\n",
    "* Pedregosa, F., G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, et al. “Scikit-Learn: Machine Learning in Python.” Journal of Machine Learning Research 12 (2011): 2825–2830."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
